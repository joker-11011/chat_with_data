{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet groq\n",
    "%pip install --quiet langchain\n",
    "%pip install --quiet chroma-hnswlib\n",
    "%pip install --quiet HuggingFace\n",
    "%pip install --quiet -U langchain-community\n",
    "%pip install --quiet python-dotenv\n",
    "%pip install --quiet pypdf\n",
    "%pip install --quiet sentence_transformers\n",
    "%pip install --quiet chromadb\n",
    "%pip install --quiet langchain_groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Nt-NmfzirKZN",
    "outputId": "2bbf76b6-801d-4f0a-ae19-a0eae7df2a83"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_groq import ChatGroq\n",
    "from IPython.display import clear_output\n",
    "\n",
    "persist_directory = 'database/chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gjg2cfmprKZN"
   },
   "outputs": [],
   "source": [
    "!rm -rf database/chroma # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pQZ0DkJkrKZO"
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Paper.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XDGfZmKfrKZO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/sd/bjm5gr593mb__5xtlb54wqbr0000gn/T/ipykernel_19893/4004144305.py:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n"
     ]
    }
   ],
   "source": [
    "full_text = \" \".join([page.page_content for page in pages])\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = r_splitter.split_text(full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_SzxaQ9PxlK",
    "outputId": "9d82c252-5373-4016-b10b-3404718438b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_instruction=\"为这个句子生成表示以用于检索相关文章：\"\n",
    ")\n",
    "model.query_instruction = \"为这个句子生成表示以用于检索相关文章：\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pb1C4C0aYEpS"
   },
   "outputs": [],
   "source": [
    "# Assuming 'chunks' is a list of strings\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,  # Pass the list of Document objects\n",
    "    embedding=model,\n",
    "    persist_directory=persist_directory,\n",
    "    client=chromadb.Client()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Groq API key\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "model = 'llama3-8b-8192'\n",
    "# Initialize Groq Langchain chat object and conversation\n",
    "groq_chat = ChatGroq(\n",
    "        groq_api_key=groq_api_key, \n",
    "        model_name=model\n",
    ")\n",
    "\n",
    "conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sd/bjm5gr593mb__5xtlb54wqbr0000gn/T/ipykernel_19893/2764353886.py:32: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  conversation = LLMChain(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is cnn? \n",
      "Chatbot: A CNN (Convolutional Neural Network) is a type of neural network that is particularly well-suited for analyzing data that has a grid-like or spatial structure, such as images or videos.\n",
      "Question: what is MFCC? \n",
      "Chatbot: There is no mention of MFCC in the provided context.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    clear_output(wait=True)\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question: \")\n",
    "        \n",
    "        # If the user has asked a question,\n",
    "        if user_question:\n",
    "            context = vectordb.max_marginal_relevance_search(user_question)\n",
    "            PROMPT_TEMPLATE = \"\"\"\n",
    "            Don't display anything non relavent to user_question.\n",
    "            Answer the question based on the following context: {context}.\n",
    "            Provide a detailed answer.\n",
    "            Don’t justify your answers.\n",
    "            Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
    "            Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
    "            \"\"\"\n",
    "\n",
    "            # Load retrieved context and user query in the prompt template\n",
    "            prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "            formatted_prompt = prompt_template.format(context=context)\n",
    "\n",
    "            # Construct a chat prompt template using various components\n",
    "            prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    SystemMessage(content=formatted_prompt),\n",
    "                    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                    HumanMessagePromptTemplate.from_template(\"{human_input}\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create a conversation chain using the LangChain LLM\n",
    "            conversation = LLMChain(\n",
    "                llm=groq_chat,\n",
    "                prompt=prompt,\n",
    "                verbose=False,\n",
    "                memory=memory,\n",
    "            )\n",
    "\n",
    "            # The chatbot's answer is generated by sending the full prompt to the Groq API.\n",
    "            response = conversation.predict(human_input=user_question)\n",
    "            print(\"Question:\", user_question, \"\\nChatbot:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
